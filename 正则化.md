## 正则化是什么

机器学习中的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。 在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。 这些策略被统称为正则化。

## 正则化的一般做法

许多正则化方法通过对目标函数 $J$添加一个**参数范数惩罚**$\Omega(\theta)$，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。
我们将正则化后的目标函数记为
$$
\tilde{J}：\tilde{J}(\theta;X, y) = J(\theta;X, y) + \alpha \Omega(\theta),
$$

其中$\alpha \in [0, \infty)$是权衡范数惩罚项$\Omega$和标准目标函数 $J(X;\theta)$相对贡献的超参数。
将$\alpha$设为0表示没有正则化。
$\alpha$越大，对应正则化惩罚越大。

## 为什么一般只正则化权重

在探究不同范数的正则化表现之前，我们需要说明一下，在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常**只对权重做惩罚而不对偏置做正则惩罚**。

 精确拟合偏置所需的数据通常比拟合权重少得多。 每个权重会指定两个变量如何相互作用。 我们需要在各种条件下观察这两个变量才能良好地拟合权重。 **而每个偏置仅控制一个单变量**。

 这意味着，我们**不对其进行正则化也不会导致太大的方差**。 另外，正则化偏置参数可能会导致明显的欠拟合。 因此，我们使用向量$w$表示所有应受范数惩罚影响的权重，而向量$\theta$表示所有参数(包括$w$和无需正则化的参数)。

以上摘自《深度学习》的中文版https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/

## L2正则化/权重衰减

而常用的**参数范数惩罚手段**有两种

一般英文称作L1-norm和L2-norm

中文称作**L1正则化**和**L2正则化**，或者L1范数和L2范数，L2正则化也被称为岭回归或者权重衰减

这里主要介绍L2正则化

L2 正则化公式非常简单，直接在原来的损失函数基础上加上权重参数的平方和

$$Loss=Loss+\lambda(\sum_{j=1}^{n}{W}_j^2)\\n代表权重w的个数$$

而正则惩罚项$$\lambda(\sum_{j=1}^{n}{W}_j^2)$$对单个权重求偏导

可以得到

$$\frac{\lambda}{m}W_j$$

因此算梯度的时候，对于L2正则化，直接加$$权重*\frac{\lambda}{m}$$即可，详见代码

## 代码实测

最终代码如下：

```python
from torchvision import datasets, transforms
import torch.utils.data as Data
import numpy as np
from tqdm._tqdm import trange
batch_size = 64
learning_rate=0.1
lambd = 3 #正则化惩罚系数
def load_data():


    # 加载torchvision包内内置的MNIST数据集 这里涉及到transform:将图片转化成torchtensor
    train_dataset = datasets.MNIST(root='./data/', train=True, transform=transforms.ToTensor(), download=True)
    test_dataset = datasets.MNIST(root='./data/', train=False, transform=transforms.ToTensor())

    # 加载小批次数据，即将MNIST数据集中的data分成每组batch_size的小块，shuffle指定是否随机读取
    train_loader = Data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = Data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader,test_loader

def train(train_loader):
    list=[]
    w=np.random.normal(scale=0.01,size=(784,10)) #生成随机正太分布的w矩阵
    b=np.zeros((batch_size,10)) #生成全是0的b矩阵
    for i in trange(0,100):
        for batch_idx, (data, target) in enumerate(train_loader):

            if(data.shape[0]<batch_size):
                break
            data= np.squeeze(data.numpy()).reshape(batch_size,784) # 把张量中维度为1的维度去掉,并且改变维度为(64,784)

            target = target.numpy() #x矩阵 (64,784)

            y_hat=softmax(np.dot(data,w)+b)#预测结果y_hat
            # print(sum(y_hat[0]))
            w_=GradientOfCrossEntropyLoss(data,target,y_hat,type="weight")#计算交叉熵损失函数对于w的梯度
            b_=GradientOfCrossEntropyLoss(data,target,y_hat,type="bias")#计算交叉熵损失函数对于b的梯度
            w=GradientDescent(w,w_+GradientOfL2(w))  #梯度下降优化参数w，权重需要正则化
            b=GradientDescent(b,b_)  #梯度下降优化参数b,偏置不需要正则化
            list.append(Accuracy(target, y_hat))
            if (batch_idx == 50):
                print("准确率为"+str(Accuracy(target, y_hat)))
                print("w的均值为"+str(w.mean()))
    return list

def softmax(label):

    label = np.exp(label.T)#先把每个元素都进行exp运算
    # print(label)
    sum = label.sum(axis=0) #对于每一行进行求和操作

    #print((label/sum).T.sum(axis=1))
    return (label/sum).T #通过广播机制，使每行分别除以各种的和

#梯度下降
def GradientDescent(Param,GradientOfLoss):
    Param = Param -GradientOfLoss/ batch_size * learning_rate
    # print(w_)
    return Param

#计算交叉熵损失函数对于w或者b的梯度
def GradientOfCrossEntropyLoss(data,y,y_hat,type="weight"):
    y = np.eye(10)[y]  # 改为one-hot形式
    data=data.T #(784,64)
    #y_hat-target为预测值与实际值的one-hot挨个做差，得出一个(64,10)的y_hat-y的矩阵

    if (type == "bias"):
        return  y_hat-y
    else:
        return np.dot(data,y_hat-y)

def GradientOfL2(matrix):
    return lambd*matrix
#正则化项是把每个参数平方和，而每个参数对正则化项求偏导都等于它自身

def Accuracy(target,y_hat):
    #y_hat.argmax(axis=1)==target 用于比较y_hat与target的每个元素，返回一个布尔数组
    acc=y_hat.argmax(axis=1) == target
    acc=acc+0  #将布尔数组转为0，1数组
    return acc.mean() #通过求均值算出准确率

if __name__ == '__main__':

    train_loader,test_loader=load_data()    #加载数据(这里使用pytorch加载数据，后面用numpy手写)
    list=train(train_loader)
    import matplotlib.pyplot as plt

    plt.plot(list)
    plt.show()
```

在上一篇关于手写softmax的博客上写的基础上改动了以下三处

```python
w=GradientDescent(w,w_+GradientOfL2(w))  #梯度下降优化参数w，权重需要正则化
```

```python
lambd = 3 #正则化惩罚系数
```

```python
def GradientOfL2(matrix):
    return lambd*matrix
#正则化项是把每个参数平方和，而每个参数对正则化项求偏导都等于它自身
```

最终

lambd为0时，w的均值为w的均值为0.0001364341718642295左右

lambd=3时，w的均值为2.4759637367577133e-19 左右

可以看出对于权重的衰减相当明显

## PS：为什么正则化可以防止过拟合

https://www.zhihu.com/question/20700829/answer/16395087

过拟合的时候，拟合函数的系数往往非常大，而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。
说说为什么过拟合的时候系数会很大。
如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。

![image][link1]





[link1]:data:image/jpg;base64,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